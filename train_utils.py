import os
import numpy as np
import tensorflow as tf
from pathlib import Path
import shutil
from sklearn.utils.class_weight import compute_class_weight

# ============================================
# ğŸ“¦ × ×™×”×•×œ ×ª×™×§×™×•×ª ×•×’×¨×¡××•×ª
# ============================================

MODELS_DIR = "models"
LATEST_MODEL = os.path.join(MODELS_DIR, "latest.h5")


def create_new_version_dir():
    """
    ×™×•×¦×¨ ×ª×™×§×™×™×” ×—×“×©×” models/vX ×¢×‘×•×¨ ×”××•×“×œ ×”×‘×.
    """
    os.makedirs(MODELS_DIR, exist_ok=True)

    versions = [
        int(f.name.replace("v", ""))
        for f in Path(MODELS_DIR).glob("v*")
        if f.is_dir() and f.name.replace("v", "").isdigit()
    ]

    next_v = max(versions) + 1 if versions else 1

    version_dir = Path(MODELS_DIR) / f"v{next_v}"
    version_dir.mkdir(exist_ok=True)

    print(f"âœ” Created version directory â†’ {version_dir}")
    return version_dir


def save_final_model(model, version_dir):
    """
    ×©×•××¨ ×¨×§ ××ª ××•×“×œ ×”-Fine-Tuning ×‘×ª×•×š ×”×ª×™×§×™×™×” vX
    ×•××¢×“×›×Ÿ ××ª latest.h5 ×‘×”×ª××.
    """
    out_path = version_dir / "model.h5"
    model.save(out_path)
    shutil.copy(out_path, LATEST_MODEL)

    print(f"âœ” Saved FINAL model â†’ {out_path}")
    print(f"âœ” Updated latest model â†’ {LATEST_MODEL}")

    return out_path


# ============================================
# ğŸš ×œ×•×’×™×§×ª Class Weights
# ============================================

def compute_balanced_class_weights(y_idx_train):
    """
    ××—×©×‘ class_weight ×××•×–×Ÿ ×œ×¤×™ sklearn.
    """
    cw = compute_class_weight(
        class_weight="balanced",
        classes=np.unique(y_idx_train),
        y=y_idx_train
    )
    class_weights = {i: float(w) for i, w in enumerate(cw)}

    print("Computed class weights:", class_weights)
    return class_weights


# ============================================
# ğŸ› Cosine Learning Rate + Warmup
# ============================================

def cosine_warmup_scheduler(initial_lr, total_epochs, warmup_epochs=3, min_lr=1e-6):
    """
    ××—×–×™×¨ callback ×©××‘×¦×¢:
    - warmup ×‘×©×œ×•×©×ª ×”××¤×•×§×™× ×”×¨××©×•× ×™×
    - ×•××– cosine decay ×¢×“ ×¡×•×£ ×”××™××•×Ÿ
    """

    def scheduler(epoch, lr):
        # ×©×œ×‘ warmup
        if epoch < warmup_epochs:
            return initial_lr * float(epoch + 1) / warmup_epochs

        # ×©×œ×‘ cosine decay
        progress = (epoch - warmup_epochs) / max(1, total_epochs - warmup_epochs)
        cosine = 0.5 * (1 + np.cos(np.pi * progress))
        return min_lr + (initial_lr - min_lr) * cosine

    return tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)


# ============================================
# ğŸ“œ ×œ×•×’×™× ×œ××™××•×Ÿ (××•×¤×¦×™×•× ×œ×™)
# ============================================

def write_training_log(path, info_dict):
    """
    ×›×•×ª×‘ training_log.txt ×¢× ××™×“×¢ ×¢×œ ×”××™××•×Ÿ:
    - ×–×× ×™×
    - class_weights
    - ×‘×™×¦×•×¢×™×
    - ×”×™×¤×¨-×¤×¨××˜×¨×™×
    """
    with open(path, "w", encoding="utf-8") as f:
        for k, v in info_dict.items():
            f.write(f"{k}: {v}\n")

    print(f"âœ” Training log saved â†’ {path}")
